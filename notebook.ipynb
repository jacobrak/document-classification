{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transcriber's Note:\\nEvery effort has been mad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“In a few moments Marianne, Solomin, Paul,\\n\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“Marianne knelt beside the sofa.… Nezhdanof\\n\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“Solomin raised Marianne's hand, her head\\n\\nl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“He now was no longer, but the hands of\\n\\nSol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Transcriber's Note:\\nEvery effort has been mad...      0\n",
       "1  “In a few moments Marianne, Solomin, Paul,\\n\\n...      0\n",
       "2  “Marianne knelt beside the sofa.… Nezhdanof\\n\\...      0\n",
       "3  “Solomin raised Marianne's hand, her head\\n\\nl...      0\n",
       "4  “He now was no longer, but the hands of\\n\\nSol...      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd \n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"story_database\"]  # Database name\n",
    "collection = db[\"short_stories\"]\n",
    "\n",
    "documents = collection.find()\n",
    "\n",
    "df_raw = pd.DataFrame(list(documents))\n",
    "df_raw = df_raw[[\"text\", \"label\"]]\n",
    "df_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cleaning function\n",
    "def clean_text(text):\n",
    "\n",
    "    text = text.replace('\\n', ' ') \n",
    "    text = re.sub(r'=', '', text)  \n",
    "\n",
    "    # Keep only English alphabet characters and spaces \n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "df_raw['cleaned_text'] = df_raw['text'].apply(clean_text)\n",
    "\n",
    "df = df_raw[[\"cleaned_text\", \"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.68      0.81        37\n",
      "           1       0.81      1.00      0.89        50\n",
      "\n",
      "    accuracy                           0.86        87\n",
      "   macro avg       0.90      0.84      0.85        87\n",
      "weighted avg       0.89      0.86      0.86        87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "X = df_shuffled[\"cleaned_text\"]\n",
    "y = df_shuffled[\"label\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)  # max_iter to ensure convergence\n",
    "clf.fit(X_train, y_train)\n",
    "        \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "cleaned_data = (classification_report(y_test, y_pred))\n",
    "print(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow this was a bit anticlimactic, lets mess the data up see if we get the same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets not clean the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.81      0.88        37\n",
      "           1       0.88      0.98      0.92        50\n",
      "\n",
      "    accuracy                           0.91        87\n",
      "   macro avg       0.92      0.90      0.90        87\n",
      "weighted avg       0.91      0.91      0.91        87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_shuffled = df_raw.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = df_shuffled[\"cleaned_text\"]\n",
    "y = df_shuffled[\"label\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)  # max_iter to ensure convergence\n",
    "clf.fit(X_train, y_train)\n",
    "        \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets add data, fakenews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakenews = pd.read_csv(\"data/fake_news.csv\")\n",
    "fakenews.columns = [\"cleaned_text\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([fakenews, df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.76      0.86        42\n",
      "           1       0.81      0.98      0.89        49\n",
      "           2       0.89      0.89      0.89        46\n",
      "\n",
      "    accuracy                           0.88       137\n",
      "   macro avg       0.90      0.88      0.88       137\n",
      "weighted avg       0.90      0.88      0.88       137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_shuffled = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = df_shuffled[\"cleaned_text\"]\n",
    "y = df_shuffled[\"label\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)  # max_iter to ensure convergence\n",
    "clf.fit(X_train, y_train)\n",
    "        \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32,  6,  4],\n",
       "       [ 0, 48,  1],\n",
       "       [ 0,  5, 41]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is the vectorizer that good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_news = pd.read_csv(\"data/bbc_news.csv\")\n",
    "bbc = bbc_news[[\"cleaned_text\", \"category_encoded\"]]\n",
    "bbc.columns =  [\"cleaned_text\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([bbc, data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "3    100\n",
       "6    100\n",
       "5    100\n",
       "7    100\n",
       "4    100\n",
       "2    100\n",
       "1    100\n",
       "0     74\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92        21\n",
      "           1       0.59      0.90      0.72        21\n",
      "           2       0.83      0.69      0.75        29\n",
      "           3       0.97      0.93      0.95        30\n",
      "           4       0.91      0.91      0.91        34\n",
      "           5       0.86      0.94      0.90        33\n",
      "           6       0.96      0.90      0.93        29\n",
      "           7       1.00      0.92      0.96        36\n",
      "\n",
      "    accuracy                           0.88       233\n",
      "   macro avg       0.89      0.88      0.88       233\n",
      "weighted avg       0.90      0.88      0.89       233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_shuffled = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = df_shuffled[\"cleaned_text\"]\n",
    "y = df_shuffled[\"label\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)  # max_iter to ensure convergence\n",
    "clf.fit(X_train, y_train)\n",
    "        \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18,  1,  2,  0,  0,  0,  0,  0],\n",
       "       [ 0, 19,  1,  0,  0,  0,  1,  0],\n",
       "       [ 0,  4, 20,  0,  3,  2,  0,  0],\n",
       "       [ 0,  1,  0, 28,  0,  1,  0,  0],\n",
       "       [ 0,  3,  0,  0, 31,  0,  0,  0],\n",
       "       [ 0,  1,  0,  1,  0, 31,  0,  0],\n",
       "       [ 0,  0,  1,  0,  0,  2, 26,  0],\n",
       "       [ 0,  3,  0,  0,  0,  0,  0, 33]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing the datas dimensions by adding more data as well as adding more things to classify is instead of worsening the model its getting improved?\n",
    "The TfidfVectorizer proves to be an excellent tool for vectorizing text data, as it does a near-perfect job of capturing the essence of the information. However, it's important to note that a vectorizer like TfidfVectorizer thrives on large datasets. While adding more labels was part of the improvement, it was the accompanying increase in data that enabled the creation of a more complex sparse matrix, allowing the model to perform more efficiently and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets create a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deeplearning = df_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca189c998cf94bc680378a7a276995c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\felik\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fa93b5a9c34ae88cae542e3267ceec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a2d4ce9c0a4c05930315032d29dd9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76decaae84b7466ca3cadcd1ac12ddb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a tokenizer — you can use any pretrained transformer model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Use tokenizer to encode the cleaned_text column\n",
    "df_deeplearning['input_ids'] = df_deeplearning['cleaned_text'].apply(\n",
    "    lambda x: tokenizer.encode(x, truncation=True, padding='max_length', max_length=128)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>label</th>\n",
       "      <th>token</th>\n",
       "      <th>input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GE sees excellent world economy  US behemoth G...</td>\n",
       "      <td>3</td>\n",
       "      <td>[GE, sees, excellent, world, economy, US, behe...</td>\n",
       "      <td>[101, 16216, 5927, 6581, 2088, 4610, 2149, 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stuart joins Norwich from Addicks  Norwich hav...</td>\n",
       "      <td>7</td>\n",
       "      <td>[Stuart, joins, Norwich, from, Addicks, Norwic...</td>\n",
       "      <td>[101, 6990, 9794, 12634, 2013, 5587, 6799, 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Movie body hits peer-to-peer nets  The movie i...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Movie, body, hits, peer-to-peer, nets, The, m...</td>\n",
       "      <td>[101, 3185, 2303, 4978, 8152, 1011, 2000, 1011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arnesen denies rift with Santini  Tottenham sp...</td>\n",
       "      <td>7</td>\n",
       "      <td>[Arnesen, denies, rift, with, Santini, Tottenh...</td>\n",
       "      <td>[101, 12098, 14183, 2078, 23439, 16931, 2007, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who do you think you are?  The real danger is ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Who, do, you, think, you, are?, The, real, da...</td>\n",
       "      <td>[101, 2040, 2079, 2017, 2228, 2017, 2024, 1029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>“…In 5 cities 8 kings; they ruled for 241,200 ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[“…In, 5, cities, 8, kings;, they, ruled, for,...</td>\n",
       "      <td>[101, 1523, 1529, 1999, 1019, 3655, 1022, 5465...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>Analysis: No pain, no gain?  He called it his ...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Analysis:, No, pain,, no, gain?, He, called, ...</td>\n",
       "      <td>[101, 4106, 1024, 2053, 3255, 1010, 2053, 5114...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>Saudi ministry to employ women  Women will be ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[Saudi, ministry, to, employ, women, Women, wi...</td>\n",
       "      <td>[101, 8174, 3757, 2000, 12666, 2308, 2308, 209...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>The mayor of a small city receives suddenly th...</td>\n",
       "      <td>0</td>\n",
       "      <td>[The, mayor, of, a, small, city, receives, sud...</td>\n",
       "      <td>[101, 1996, 3664, 1997, 1037, 2235, 2103, 8267...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>Newcastle 2-1 Bolton  Kieron Dyer smashed home...</td>\n",
       "      <td>7</td>\n",
       "      <td>[Newcastle, 2-1, Bolton, Kieron, Dyer, smashed...</td>\n",
       "      <td>[101, 8142, 1016, 1011, 1015, 12118, 11382, 26...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>774 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          cleaned_text  label  \\\n",
       "0    GE sees excellent world economy  US behemoth G...      3   \n",
       "1    Stuart joins Norwich from Addicks  Norwich hav...      7   \n",
       "2    Movie body hits peer-to-peer nets  The movie i...      4   \n",
       "3    Arnesen denies rift with Santini  Tottenham sp...      7   \n",
       "4    Who do you think you are?  The real danger is ...      4   \n",
       "..                                                 ...    ...   \n",
       "769  “…In 5 cities 8 kings; they ruled for 241,200 ...      2   \n",
       "770  Analysis: No pain, no gain?  He called it his ...      5   \n",
       "771  Saudi ministry to employ women  Women will be ...      3   \n",
       "772  The mayor of a small city receives suddenly th...      0   \n",
       "773  Newcastle 2-1 Bolton  Kieron Dyer smashed home...      7   \n",
       "\n",
       "                                                 token  \\\n",
       "0    [GE, sees, excellent, world, economy, US, behe...   \n",
       "1    [Stuart, joins, Norwich, from, Addicks, Norwic...   \n",
       "2    [Movie, body, hits, peer-to-peer, nets, The, m...   \n",
       "3    [Arnesen, denies, rift, with, Santini, Tottenh...   \n",
       "4    [Who, do, you, think, you, are?, The, real, da...   \n",
       "..                                                 ...   \n",
       "769  [“…In, 5, cities, 8, kings;, they, ruled, for,...   \n",
       "770  [Analysis:, No, pain,, no, gain?, He, called, ...   \n",
       "771  [Saudi, ministry, to, employ, women, Women, wi...   \n",
       "772  [The, mayor, of, a, small, city, receives, sud...   \n",
       "773  [Newcastle, 2-1, Bolton, Kieron, Dyer, smashed...   \n",
       "\n",
       "                                             input_ids  \n",
       "0    [101, 16216, 5927, 6581, 2088, 4610, 2149, 202...  \n",
       "1    [101, 6990, 9794, 12634, 2013, 5587, 6799, 201...  \n",
       "2    [101, 3185, 2303, 4978, 8152, 1011, 2000, 1011...  \n",
       "3    [101, 12098, 14183, 2078, 23439, 16931, 2007, ...  \n",
       "4    [101, 2040, 2079, 2017, 2228, 2017, 2024, 1029...  \n",
       "..                                                 ...  \n",
       "769  [101, 1523, 1529, 1999, 1019, 3655, 1022, 5465...  \n",
       "770  [101, 4106, 1024, 2053, 3255, 1010, 2053, 5114...  \n",
       "771  [101, 8174, 3757, 2000, 12666, 2308, 2308, 209...  \n",
       "772  [101, 1996, 3664, 1997, 1037, 2235, 2103, 8267...  \n",
       "773  [101, 8142, 1016, 1011, 1015, 12118, 11382, 26...  \n",
       "\n",
       "[774 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_deeplearning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
