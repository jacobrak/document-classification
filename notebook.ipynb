{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transcriber's Note:\\nEvery effort has been mad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“In a few moments Marianne, Solomin, Paul,\\n\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“Marianne knelt beside the sofa.… Nezhdanof\\n\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“Solomin raised Marianne's hand, her head\\n\\nl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“He now was no longer, but the hands of\\n\\nSol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Transcriber's Note:\\nEvery effort has been mad...      0\n",
       "1  “In a few moments Marianne, Solomin, Paul,\\n\\n...      0\n",
       "2  “Marianne knelt beside the sofa.… Nezhdanof\\n\\...      0\n",
       "3  “Solomin raised Marianne's hand, her head\\n\\nl...      0\n",
       "4  “He now was no longer, but the hands of\\n\\nSol...      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd \n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"story_database\"]  # Database name\n",
    "collection = db[\"short_stories\"]\n",
    "\n",
    "documents = collection.find()\n",
    "\n",
    "df_raw = pd.DataFrame(list(documents))\n",
    "df_raw = df_raw[[\"text\", \"label\"]]\n",
    "df_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cleaning function\n",
    "def clean_text(text):\n",
    "\n",
    "    text = text.replace('\\n', ' ') \n",
    "    text = re.sub(r'=', '', text)  \n",
    "\n",
    "    # Keep only English alphabet characters and spaces \n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "df_raw['cleaned_text'] = df_raw['text'].apply(clean_text)\n",
    "\n",
    "df = df_raw[[\"cleaned_text\", \"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.74      0.85        42\n",
      "           1       0.80      1.00      0.89        45\n",
      "\n",
      "    accuracy                           0.87        87\n",
      "   macro avg       0.90      0.87      0.87        87\n",
      "weighted avg       0.90      0.87      0.87        87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "X = df_shuffled[\"cleaned_text\"]\n",
    "y = df_shuffled[\"label\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)  # max_iter to ensure convergence\n",
    "clf.fit(X_train, y_train)\n",
    "        \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "cleaned_data = (classification_report(y_test, y_pred))\n",
    "print(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow this was a bit anticlimactic, lets mess the data up see if we get the same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets not clean the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93        40\n",
      "           1       0.90      1.00      0.95        47\n",
      "\n",
      "    accuracy                           0.94        87\n",
      "   macro avg       0.95      0.94      0.94        87\n",
      "weighted avg       0.95      0.94      0.94        87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_shuffled = df_raw.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = df_shuffled[\"cleaned_text\"]\n",
    "y = df_shuffled[\"label\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)  # max_iter to ensure convergence\n",
    "clf.fit(X_train, y_train)\n",
    "        \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets add data, fakenews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakenews = pd.read_csv(\"data/fake_news.csv\")\n",
    "fakenews.columns = [\"cleaned_text\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([fakenews, df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        34\n",
      "           1       0.86      1.00      0.93        50\n",
      "           2       0.98      0.85      0.91        53\n",
      "\n",
      "    accuracy                           0.93       137\n",
      "   macro avg       0.95      0.94      0.94       137\n",
      "weighted avg       0.94      0.93      0.93       137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_shuffled = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = df_shuffled[\"cleaned_text\"]\n",
    "y = df_shuffled[\"label\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)  # max_iter to ensure convergence\n",
    "clf.fit(X_train, y_train)\n",
    "        \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33,  0,  1],\n",
       "       [ 0, 50,  0],\n",
       "       [ 0,  8, 45]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is the vectorizer that good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_news = pd.read_csv(\"data/bbc_news.csv\")\n",
    "bbc = bbc_news[[\"cleaned_text\", \"category_encoded\"]]\n",
    "bbc.columns =  [\"cleaned_text\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([bbc, data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "3    100\n",
       "6    100\n",
       "5    100\n",
       "7    100\n",
       "4    100\n",
       "2    100\n",
       "1    100\n",
       "0     74\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.93        22\n",
      "           1       0.76      0.90      0.82        31\n",
      "           2       0.88      0.77      0.82        30\n",
      "           3       0.92      0.85      0.88        27\n",
      "           4       0.89      1.00      0.94        24\n",
      "           5       0.93      0.91      0.92        43\n",
      "           6       0.86      0.92      0.89        26\n",
      "           7       1.00      0.97      0.98        30\n",
      "\n",
      "    accuracy                           0.90       233\n",
      "   macro avg       0.90      0.90      0.90       233\n",
      "weighted avg       0.90      0.90      0.90       233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_shuffled = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = df_shuffled[\"cleaned_text\"]\n",
    "y = df_shuffled[\"label\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)  # max_iter to ensure convergence\n",
    "clf.fit(X_train, y_train)\n",
    "        \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19,  2,  1,  0,  0,  0,  0,  0],\n",
       "       [ 0, 28,  0,  0,  1,  0,  2,  0],\n",
       "       [ 0,  4, 23,  1,  1,  1,  0,  0],\n",
       "       [ 0,  2,  1, 23,  0,  1,  0,  0],\n",
       "       [ 0,  0,  0,  0, 24,  0,  0,  0],\n",
       "       [ 0,  0,  1,  1,  0, 39,  2,  0],\n",
       "       [ 0,  0,  0,  0,  1,  1, 24,  0],\n",
       "       [ 0,  1,  0,  0,  0,  0,  0, 29]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing the datas dimensions by adding more data as well as adding more things to classify is instead of worsening the model its getting improved?\n",
    "The TfidfVectorizer proves to be an excellent tool for vectorizing text data, as it does a near-perfect job of capturing the essence of the information. However, it's important to note that a vectorizer like TfidfVectorizer thrives on large datasets. While adding more labels was part of the improvement, it was the accompanying increase in data that enabled the creation of a more complex sparse matrix, allowing the model to perform more efficiently and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets create a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deeplearning = df_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Use tokenizer to encode\n",
    "df_deeplearning['input_ids'] = df_deeplearning['cleaned_text'].apply(\n",
    "    lambda x: tokenizer.encode(x, truncation=True, padding='max_length', max_length=128)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>label</th>\n",
       "      <th>input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Koubek suspended after drugs test  Stefan Koub...</td>\n",
       "      <td>7</td>\n",
       "      <td>[101, 12849, 12083, 5937, 6731, 2044, 5850, 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Uganda bans Vagina Monologues  Ugandas authori...</td>\n",
       "      <td>6</td>\n",
       "      <td>[101, 10031, 7221, 2015, 12436, 20876, 18847, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trust ye therefore your heart ere you trust yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 3404, 6300, 3568, 2115, 2540, 9413, 2063...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BBC web search aids odd queries  The BBCs onli...</td>\n",
       "      <td>4</td>\n",
       "      <td>[101, 4035, 4773, 3945, 8387, 5976, 10861, 513...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ruthenium hexafluoride also rutheniumVI fluori...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 7920, 18595, 2819, 2002, 18684, 10258, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>LOS ANGELES (Reuters) - Editors Note: Attentio...</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 3050, 3349, 1006, 26665, 1007, 1011, 101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>Pupils to get anti-piracy lessons  Lessons on ...</td>\n",
       "      <td>6</td>\n",
       "      <td>[101, 7391, 2000, 2131, 3424, 1011, 24386, 822...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>Russia WTO talks make progress  Talks on Russi...</td>\n",
       "      <td>3</td>\n",
       "      <td>[101, 3607, 1059, 3406, 7566, 2191, 5082, 7566...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>iTunes now selling Band Aid song  Ipod owners ...</td>\n",
       "      <td>6</td>\n",
       "      <td>[101, 11943, 2085, 4855, 2316, 4681, 2299, 263...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>Meanwhile along with the evening was approachi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 5564, 2247, 2007, 1996, 3944, 2001, 8455...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>774 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          cleaned_text  label  \\\n",
       "0    Koubek suspended after drugs test  Stefan Koub...      7   \n",
       "1    Uganda bans Vagina Monologues  Ugandas authori...      6   \n",
       "2    Trust ye therefore your heart ere you trust yo...      0   \n",
       "3    BBC web search aids odd queries  The BBCs onli...      4   \n",
       "4    Ruthenium hexafluoride also rutheniumVI fluori...      1   \n",
       "..                                                 ...    ...   \n",
       "769  LOS ANGELES (Reuters) - Editors Note: Attentio...      2   \n",
       "770  Pupils to get anti-piracy lessons  Lessons on ...      6   \n",
       "771  Russia WTO talks make progress  Talks on Russi...      3   \n",
       "772  iTunes now selling Band Aid song  Ipod owners ...      6   \n",
       "773  Meanwhile along with the evening was approachi...      0   \n",
       "\n",
       "                                             input_ids  \n",
       "0    [101, 12849, 12083, 5937, 6731, 2044, 5850, 32...  \n",
       "1    [101, 10031, 7221, 2015, 12436, 20876, 18847, ...  \n",
       "2    [101, 3404, 6300, 3568, 2115, 2540, 9413, 2063...  \n",
       "3    [101, 4035, 4773, 3945, 8387, 5976, 10861, 513...  \n",
       "4    [101, 7920, 18595, 2819, 2002, 18684, 10258, 1...  \n",
       "..                                                 ...  \n",
       "769  [101, 3050, 3349, 1006, 26665, 1007, 1011, 101...  \n",
       "770  [101, 7391, 2000, 2131, 3424, 1011, 24386, 822...  \n",
       "771  [101, 3607, 1059, 3406, 7566, 2191, 5082, 7566...  \n",
       "772  [101, 11943, 2085, 4855, 2316, 4681, 2299, 263...  \n",
       "773  [101, 5564, 2247, 2007, 1996, 3944, 2001, 8455...  \n",
       "\n",
       "[774 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_deeplearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(df_deeplearning['input_ids'].tolist(), dtype=torch.long)\n",
    "labels = torch.tensor(df_deeplearning['label'].tolist(), dtype=torch.float)  # Assuming multi-label\n",
    "\n",
    "# Create torch dataset\n",
    "dataset = TensorDataset(input_ids, labels)\n",
    "\n",
    "train_length=int(0.7* len(dataset))\n",
    "\n",
    "test_length=len(dataset)-train_length\n",
    "\n",
    "train_dataset, test_dataset=torch.utils.data.random_split(dataset,(train_length,test_length))\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units, output_size):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(input_size, hidden_units))\n",
    "        self.layer2 = nn.Linear(hidden_units, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.layer2(x)  \n",
    "        return x\n",
    "    \n",
    "\n",
    "model_0 = SimpleModel(input_size=128, hidden_units=16, output_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([774, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_0.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Average Training Loss: 142.1564\n",
      "Epoch 21/100, Average Training Loss: 2.0779\n",
      "Epoch 41/100, Average Training Loss: 2.0767\n",
      "Epoch 61/100, Average Training Loss: 2.0772\n",
      "Epoch 81/100, Average Training Loss: 2.0766\n"
     ]
    }
   ],
   "source": [
    "def training_loop(loss_fn, optimizer, model,epochs=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            \n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            X = X.float() \n",
    "            y = y.long()  \n",
    "            \n",
    "            y_pred = model_0(X)\n",
    "\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        \n",
    "        train_loss /= len(train_dataloader)\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Average Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Call the training loop\n",
    "training_loop(loss_fn=loss_fn, optimizer=optimizer, model=model_0, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoreAdvancedModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(input_size, hidden_units))\n",
    "        self.layer2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.layer3 = nn.Linear(hidden_units, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.layer3(x)  \n",
    "        return x\n",
    "    \n",
    "\n",
    "model_1 = MoreAdvancedModel(input_size=128, hidden_units=512, output_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_1.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Average Training Loss: 2.0749\n",
      "Epoch 21/100, Average Training Loss: 2.0749\n",
      "Epoch 41/100, Average Training Loss: 2.0755\n",
      "Epoch 61/100, Average Training Loss: 2.0756\n",
      "Epoch 81/100, Average Training Loss: 2.0755\n"
     ]
    }
   ],
   "source": [
    "training_loop(loss_fn=loss_fn, optimizer=optimizer, model=model_1, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "true_preds = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_1.to(device)\n",
    "model_1.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs = data[0].to(device).float()  \n",
    "        outputs = model_1(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)  # or outputs if you want raw logits/probs\n",
    "        all_preds.append(preds)\n",
    "        true_preds.append(data[1])\n",
    "\n",
    "true_preds = torch.cat(true_preds).tolist()\n",
    "final_preds = torch.cat(all_preds).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  0,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  5,\n",
       "  1,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  7,\n",
       "  0,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  7,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  5,\n",
       "  0,\n",
       "  5,\n",
       "  7,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  4,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  7,\n",
       "  0,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  4,\n",
       "  7,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  7,\n",
       "  5,\n",
       "  1,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  0,\n",
       "  7,\n",
       "  1,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  6,\n",
       "  7,\n",
       "  1,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  7,\n",
       "  0,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  3,\n",
       "  7,\n",
       "  1,\n",
       "  5,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  4,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  3,\n",
       "  7,\n",
       "  7,\n",
       "  4,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  6,\n",
       "  7,\n",
       "  3,\n",
       "  5,\n",
       "  4,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  1,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  5,\n",
       "  0,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  3,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  3,\n",
       "  5,\n",
       "  7],\n",
       " [0.0,\n",
       "  1.0,\n",
       "  7.0,\n",
       "  6.0,\n",
       "  4.0,\n",
       "  3.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  4.0,\n",
       "  1.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  2.0,\n",
       "  7.0,\n",
       "  3.0,\n",
       "  6.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  7.0,\n",
       "  5.0,\n",
       "  6.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  5.0,\n",
       "  7.0,\n",
       "  5.0,\n",
       "  6.0,\n",
       "  4.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  3.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  6.0,\n",
       "  7.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  6.0,\n",
       "  3.0,\n",
       "  6.0,\n",
       "  4.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  0.0,\n",
       "  6.0,\n",
       "  6.0,\n",
       "  2.0,\n",
       "  6.0,\n",
       "  5.0,\n",
       "  6.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  5.0,\n",
       "  6.0,\n",
       "  2.0,\n",
       "  7.0,\n",
       "  2.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  6.0,\n",
       "  0.0,\n",
       "  6.0,\n",
       "  0.0,\n",
       "  3.0,\n",
       "  7.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  7.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  3.0,\n",
       "  6.0,\n",
       "  2.0,\n",
       "  4.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  7.0,\n",
       "  3.0,\n",
       "  7.0,\n",
       "  4.0,\n",
       "  6.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  6.0,\n",
       "  3.0,\n",
       "  6.0,\n",
       "  3.0,\n",
       "  6.0,\n",
       "  0.0,\n",
       "  5.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  4.0,\n",
       "  0.0,\n",
       "  6.0,\n",
       "  4.0,\n",
       "  3.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  6.0,\n",
       "  6.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  6.0,\n",
       "  5.0,\n",
       "  6.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  7.0,\n",
       "  3.0,\n",
       "  4.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  4.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  5.0,\n",
       "  3.0,\n",
       "  5.0,\n",
       "  6.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  6.0,\n",
       "  3.0,\n",
       "  7.0,\n",
       "  6.0,\n",
       "  0.0,\n",
       "  5.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  5.0,\n",
       "  1.0,\n",
       "  6.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  5.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  7.0,\n",
       "  0.0,\n",
       "  3.0,\n",
       "  6.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  0.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  4.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  6.0,\n",
       "  0.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  6.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  5.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  7.0,\n",
       "  6.0,\n",
       "  0.0,\n",
       "  6.0,\n",
       "  4.0,\n",
       "  7.0,\n",
       "  6.0,\n",
       "  6.0,\n",
       "  5.0,\n",
       "  6.0,\n",
       "  2.0,\n",
       "  4.0,\n",
       "  3.0,\n",
       "  7.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  0.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  4.0,\n",
       "  6.0,\n",
       "  5.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  3.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  7.0,\n",
       "  3.0,\n",
       "  0.0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds, true_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
